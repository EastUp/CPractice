# <center>84.FFmpeg-音乐播放器2<center>

具体代码请看：**[NDKPractice项目的ffmpeg83](https://github.com/EastUp/NDKPractice/tree/master/ffmpeg83)**

# 知识点：

## 1.JNIEnv 子线程回调 Java 问题

原因：
- 子线程(pThread)用不了主线程(native线程)的 jniEnv
- 子线程是不共享 jniEnv，他们有自己所独有的

`AudioTrack在哪个线程播放就得在哪个线程创建`

解决：

```c++
// 重写 so 被加载时会调用的一个方法,在这里获取javavm
extern "C" JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM *javaVM,void *reserved){
    pJavaVM = javaVM;
    JNIEnv *env;
    if(javaVM->GetEnv((void **)&env,JNI_VERSION_1_4) != JNI_OK){
        return -1;
    }
    return JNI_VERSION_1_4;
}

// 子线程中获取 JNIEnv

// 通过 JavaVM获取当前线程的 JniEnv
JNIEnv* env;
if(javaVM->AttachCurrentThread(&env,0) != JNI_OK){
    LOGE("get child thread jniEnv error");
    return;
}
jstring jMsg = env->NewStringUTF(msg);
env->CallVoidMethod(jPlayerObj, jPlayerErrorMid, code, jMsg);
env->DeleteLocalRef(jMsg);
javaVM->DetachCurrentThread();



// 获取到env后，调用AudioTrack播放时，AudioTrack 对象也得在子线程中创建。
 if(threadMode == THREAD_CHILD){
    JNIEnv *env;
    if(pJniCall->javaVM->AttachCurrentThread(&env,nullptr) != JNI_OK){
        LOGE("get child thread jniEnv error");
        return;
    }

    // 在子线程中使用自己的env初始化AudioTrack
    /*AudioTrack(int streamType, int sampleRateInHz, int channelConfig, int audioFormat,
    int bufferSizeInBytes, int mode)*/
    jclass jAudioTrackClass = env->FindClass("android/media/AudioTrack");
    jmethodID jAudioTackCMid = env->GetMethodID(jAudioTrackClass, "<init>", "(IIIIII)V");
    int streamType = 3;
    int sampleRateInHz = AUDIO_SAMPLE_RATE;
    int channelConfig = (0x4 | 0x8);
    int audioFormat = 2;
    int mode = 1;
    // int getMinBufferSize(int sampleRateInHz, int channelConfig, int audioFormat)
    jmethodID getMinBufferSizeMid = env->GetStaticMethodID(jAudioTrackClass, "getMinBufferSize",
                                                           "(III)I");
    int bufferSizeInBytes = env->CallStaticIntMethod(jAudioTrackClass, getMinBufferSizeMid,
                                                     sampleRateInHz, channelConfig, audioFormat);
    LOGE("bufferSizeInBytes = %d", bufferSizeInBytes);

    jobject jAudioTrackObj = env->NewObject(jAudioTrackClass, jAudioTackCMid, streamType,
                                            sampleRateInHz, channelConfig, audioFormat, bufferSizeInBytes, mode);
    // start  method
    jmethodID playMid = env->GetMethodID(jAudioTrackClass, "play", "()V");
    env->CallVoidMethod(jAudioTrackObj, playMid);
    // write method
    jmethodID jAudioTrackWriteMid = env->GetMethodID(jAudioTrackClass, "write", "([BII)I");


    jbyteArray jPcmByteArray =env->NewByteArray(dataSize);
    // native 创建 c 数组
    jbyte *jPcmData = env->GetByteArrayElements(jPcmByteArray, NULL);

    pPacket = av_packet_alloc();
    pFrame = av_frame_alloc();
    // 循环从上下文中读取帧到包中
    while (av_read_frame(pFormatContext, pPacket) >= 0) {
        if (pPacket->stream_index == audioStreamIndex) {
            // Packet 包，压缩的数据，解码成 pcm 数据
            int codecSendPacketRes = avcodec_send_packet(pCodecContext, pPacket);
            if (codecSendPacketRes == 0) {
                int codecReceiveFrameRes = avcodec_receive_frame(pCodecContext, pFrame);
                if (codecReceiveFrameRes == 0) {
                    // AVPacket -> AVFrame
                    index++;
                    LOGE("解码第 %d 帧", index);

                    // 调用重采样的方法
                    swr_convert(swrContext, &resampleOutBuffer, pFrame->nb_samples,
                                (const uint8_t **) pFrame->data, pFrame->nb_samples);

                    // write 写到缓冲区 pFrame.data -> javabyte
                    // size 是多大，装 pcm 的数据
                    // 1s 44100 点，2通道， 2字节 44100*2*2
                    // 1帧不是一秒，pFrame->nb_samples点

                    memcpy(jPcmData, resampleOutBuffer, dataSize);
                    // 1 把 c 的数组的数据同步到 jbyteArray,然后不释放native数组
                    env->ReleaseByteArrayElements(jPcmByteArray, jPcmData, JNI_COMMIT);

                    env->CallIntMethod(jAudioTrackObj,jAudioTrackWriteMid,jPcmByteArray,0,dataSize);
                }
            }
        }
        // 解引用
        av_packet_unref(pPacket);
        av_frame_unref(pFrame);
    }

    // 1.解引用数据 data, 2.销魂 pPacket 结构体内存， 3.pPacket = NULL;
    av_packet_free(&pPacket);
    av_frame_free(&pFrame);
    // 0 把 c 的数组的数据同步到 jbyteArray,然后释放native数组
    env->ReleaseByteArrayElements(jPcmByteArray, jPcmData, 0);
    // 解除 jPcmDataArray 的持有，让 javaGC 回收
    env->DeleteLocalRef(jPcmByteArray);
    free(resampleOutBuffer);
    pJniCall->javaVM->DetachCurrentThread();
}else{

    pJniCall->createAudioTrack();
    ...
}
```


## 2.OpenSLES播放音频的流程

参考[谷歌demo](https://github.com/android/ndk-samples/blob/main/native-audio/app/src/main/cpp/native-audio-jni.c)

OpenSLES OpenGLES 都是自带的    
`***ES` 与 `***` 之间可以说是基本没有区别，区别就是 `***ES` 是 `***` 的精简  
而且他们都有一定规则，方法调用命名规则一般为： `sl***`() , `gl***3f`  

1. 创建引擎接口对象
2. 设置混音器及参数
3. 创建播放器
4. 设置缓存队列和回调函数
5. 设置播放状态
6. 调用回调函数

```c++
FILE *pcmFile = NULL;
void *pcmBuffer = NULL;

void playerCallback(SLAndroidSimpleBufferQueueItf caller, void *pContext) {
    // 回调函数，循环
    if (!feof(pcmFile)) {
        fread(pcmBuffer, 1, 44100 * 2 * 2, pcmFile);
        (*caller)->Enqueue(caller, pcmBuffer, 44100 * 2 * 2);
    } else {
        fclose(pcmFile);
        free(pcmBuffer);
    }
}

void initCrateOpenSLES() {
    /*OpenSLES OpenGLES 都是自带的
    XXXES 与 XXX 之间可以说是基本没有区别，区别就是 XXXES 是 XXX 的精简
    而且他们都有一定规则，命名规则 slXXX() , glXXX3f*/
    // 3.1 创建引擎接口对象
    SLObjectItf engineObject = NULL;
    SLEngineItf engineEngine;
    slCreateEngine(&engineObject, 0, NULL, 0, NULL, NULL);
    // realize the engine
    (*engineObject)->Realize(engineObject, SL_BOOLEAN_FALSE);
    // get the engine interface, which is needed in order to create other objects
    (*engineObject)->GetInterface(engineObject, SL_IID_ENGINE, &engineEngine);
    // 3.2 设置混音器
    static SLObjectItf outputMixObject = NULL;
    const SLInterfaceID ids[1] = {SL_IID_ENVIRONMENTALREVERB};
    const SLboolean req[1] = {SL_BOOLEAN_FALSE};
    (*engineEngine)->CreateOutputMix(engineEngine, &outputMixObject, 1, ids, req);
    (*outputMixObject)->Realize(outputMixObject, SL_BOOLEAN_FALSE);
    SLEnvironmentalReverbItf outputMixEnvironmentalReverb = NULL;
    (*outputMixObject)->GetInterface(outputMixObject, SL_IID_ENVIRONMENTALREVERB,
            &outputMixEnvironmentalReverb);
    SLEnvironmentalReverbSettings reverbSettings = SL_I3DL2_ENVIRONMENT_PRESET_STONECORRIDOR;
    (*outputMixEnvironmentalReverb)->SetEnvironmentalReverbProperties(outputMixEnvironmentalReverb,
            &reverbSettings);
    // 3.3 创建播放器
    SLObjectItf pPlayer = NULL;
    SLPlayItf pPlayItf = NULL;
    SLDataLocator_AndroidSimpleBufferQueue simpleBufferQueue = {
            SL_DATALOCATOR_ANDROIDSIMPLEBUFFERQUEUE, 2};
    SLDataFormat_PCM formatPcm = {
            SL_DATAFORMAT_PCM,
            2,
            SL_SAMPLINGRATE_44_1,
            SL_PCMSAMPLEFORMAT_FIXED_16,
            SL_PCMSAMPLEFORMAT_FIXED_16,
            SL_SPEAKER_FRONT_LEFT | SL_SPEAKER_FRONT_RIGHT,
            SL_BYTEORDER_LITTLEENDIAN};
    SLDataSource audioSrc = {&simpleBufferQueue, &formatPcm};
    SLDataLocator_OutputMix outputMix = {SL_DATALOCATOR_OUTPUTMIX, outputMixObject};
    SLDataSink audioSnk = {&outputMix, NULL};
    SLInterfaceID interfaceIds[3] = {SL_IID_BUFFERQUEUE, SL_IID_VOLUME, SL_IID_PLAYBACKRATE};
    SLboolean interfaceRequired[3] = {SL_BOOLEAN_TRUE, SL_BOOLEAN_TRUE, SL_BOOLEAN_TRUE};
    (*engineEngine)->CreateAudioPlayer(engineEngine, &pPlayer, &audioSrc, &audioSnk, 3,
            interfaceIds, interfaceRequired);
    (*pPlayer)->Realize(pPlayer, SL_BOOLEAN_FALSE);
    (*pPlayer)->GetInterface(pPlayer, SL_IID_PLAY, &pPlayItf);
    // 3.4 设置缓存队列和回调函数
    SLAndroidSimpleBufferQueueItf playerBufferQueue;
    (*pPlayer)->GetInterface(pPlayer, SL_IID_BUFFERQUEUE, &playerBufferQueue);
    (*playerBufferQueue)->RegisterCallback(playerBufferQueue, playerCallback, NULL);
    // 3.5 设置播放状态
    (*pPlayItf)->SetPlayState(pPlayItf, SL_PLAYSTATE_PLAYING);
    // 3.6 调用回调函数
    playerCallback(playerBufferQueue, NULL);
}
```



